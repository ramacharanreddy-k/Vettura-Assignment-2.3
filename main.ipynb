{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramfeuji/Documents/Vettura AI/Week 2/Vettura-Assignment-2.3/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from Hugging Face...\n",
      "Dataset loaded. Train size: 6241, Test size: 1759\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset from Hugging Face...\")\n",
    "dataset = load_dataset(\"cnamuangtoun/resume-job-description-fit\")\n",
    "print(f\"Dataset loaded. Train size: {len(dataset['train'])}, Test size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobSearchRAG:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.job_descriptions = []\n",
    "        self.job_embeddings = None\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean text by removing extra whitespace and normalizing newlines.\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.replace('\\\\n', '\\n').strip()\n",
    "        return text\n",
    "    \n",
    "    def extract_job_info(self, text: str, fit_label: str = None) -> Dict[str, str]:\n",
    "        \"\"\"Extract structured information from job description text.\"\"\"\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Extract job title/role\n",
    "        role_patterns = [\n",
    "            r\"(?:Job Title|Position|Role):\\s*([^\\n]+)\",\n",
    "            r\"^(?:Senior|Junior|Lead|Principal|Staff)?\\s*([A-Za-z\\s]+(?:Engineer|Developer|Analyst|Manager|Architect|Designer|Consultant|Director|Specialist))[^\\n]*\",\n",
    "            r\"([A-Za-z\\s]+(?:Engineer|Developer|Analyst|Manager|Architect|Designer|Consultant|Director|Specialist))[^\\n]*\"\n",
    "        ]\n",
    "        \n",
    "        role = \"Role Not Specified\"\n",
    "        for pattern in role_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
    "            if match:\n",
    "                role = match.group(1).strip()\n",
    "                break\n",
    "        \n",
    "        # Extract location\n",
    "        location_patterns = [\n",
    "            r\"Location:\\s*([^\\n]+)\",\n",
    "            r\"(?:based in|located in|location):\\s*([^\\n]+)\",\n",
    "            r\"(?:City|State|Country):\\s*([^\\n]+)\"\n",
    "        ]\n",
    "        \n",
    "        location = \"Location Not Specified\"\n",
    "        for pattern in location_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                location = match.group(1).strip()\n",
    "                break\n",
    "        \n",
    "        # Extract work mode\n",
    "        work_mode_patterns = [\n",
    "            r\"(?:Work Mode|Work Type|Work Location):\\s*([^\\n]+)\",\n",
    "            r\"(?:Remote|Hybrid|On-site|In-office|Virtual)(?:\\s+work|\\s+position)?\"\n",
    "        ]\n",
    "        \n",
    "        work_mode = \"Not Specified\"\n",
    "        for pattern in work_mode_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                work_mode = match.group(1).strip() if match.groups() else match.group(0).strip()\n",
    "                break\n",
    "                \n",
    "        # Create unique identifier for deduplication\n",
    "        import hashlib\n",
    "        content_hash = hashlib.md5(text.encode()).hexdigest()\n",
    "        \n",
    "        return {\n",
    "            'role': role,\n",
    "            'location': location,\n",
    "            'work_mode': work_mode,\n",
    "            'text': text,\n",
    "            'fit_label': fit_label,\n",
    "            'hash': content_hash\n",
    "        }\n",
    "\n",
    "    def process_dataset(self, dataset):\n",
    "        \"\"\"Process both train and test splits of the dataset with deduplication.\"\"\"\n",
    "        print(\"Processing dataset...\")\n",
    "        seen_hashes = set()\n",
    "        all_jobs = []\n",
    "        \n",
    "        # Process both train and test sets\n",
    "        for split in ['train', 'test']:\n",
    "            for item in dataset[split]:\n",
    "                job_text = item['job_description_text']\n",
    "                fit_label = item['label']  # Get the fit label\n",
    "                \n",
    "                if isinstance(job_text, str) and len(job_text.strip()) > 0:\n",
    "                    job_info = self.extract_job_info(job_text, fit_label)\n",
    "                    \n",
    "                    # Only add if we haven't seen this exact job before\n",
    "                    if job_info['hash'] not in seen_hashes:\n",
    "                        seen_hashes.add(job_info['hash'])\n",
    "                        # Remove hash before adding to final list\n",
    "                        del job_info['hash']\n",
    "                        all_jobs.append(job_info)\n",
    "        \n",
    "        print(f\"Found {len(all_jobs)} unique job descriptions\")\n",
    "        return all_jobs\n",
    "\n",
    "    def add_job_descriptions(self, dataset):\n",
    "        \"\"\"Process and add job descriptions from the dataset.\"\"\"\n",
    "        self.job_descriptions = self.process_dataset(dataset)\n",
    "        \n",
    "        # Create embeddings for job descriptions\n",
    "        print(\"Creating embeddings...\")\n",
    "        job_texts = [job['text'] for job in self.job_descriptions]\n",
    "        self.job_embeddings = self.model.encode(job_texts, show_progress_bar=True)\n",
    "        print(\"Embeddings created successfully!\")\n",
    "        \n",
    "    def search_jobs(self, query: str, num_results: int = 5) -> List[Dict]:\n",
    "        \"\"\"Search for jobs and return results with fit labels.\"\"\"\n",
    "        # Encode query\n",
    "        query_embedding = self.model.encode([query])[0]\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity([query_embedding], self.job_embeddings)[0]\n",
    "        \n",
    "        # Get top k matches\n",
    "        top_indices = np.argsort(similarities)[-num_results:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            job = self.job_descriptions[idx].copy()\n",
    "            job['similarity_score'] = float(similarities[idx])\n",
    "            results.append(job)\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset...\n",
      "Found 351 unique job descriptions\n",
      "Creating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 11/11 [00:02<00:00,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rag_system = JobSearchRAG()\n",
    "rag_system.add_job_descriptions(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving processed data...\")\n",
    "np.save('job_embeddings.npy', rag_system.job_embeddings)\n",
    "import pickle\n",
    "with open('job_descriptions.pkl', 'wb') as f:\n",
    "    pickle.dump(rag_system.job_descriptions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the system...\n",
      "\n",
      "Test Results:\n",
      "\n",
      "1. Role: and software test engineer\n",
      "Location: Location Not Specified\n",
      "Work Mode: Not Specified\n",
      "Fit Label: No Fit\n",
      "Similarity Score: 0.52\n",
      "--------------------------------------------------\n",
      "\n",
      "2. Role: Software Developer\n",
      "Location: Location Not Specified\n",
      "Work Mode: Hybrid\n",
      "Fit Label: No Fit\n",
      "Similarity Score: 0.51\n",
      "--------------------------------------------------\n",
      "\n",
      "3. Role: Software Data Engineer\n",
      "Location: Location Not Specified\n",
      "Work Mode: Hybrid\n",
      "Fit Label: No Fit\n",
      "Similarity Score: 0.50\n",
      "--------------------------------------------------\n",
      "\n",
      "4. Role: Data AnalystLocation: Plano, TX (Day 1 Hybrid)Duration: Long Term Contract Description: Must have skills SQL and Tableau 4 years of Data Analysis and business knowledge within a healthcare environment and testing arena3-5 years of Microsoft SQL experience specifically using SQL to build queries, reports.--\n",
      "Location: Plano, TX (Day 1 Hybrid)Duration: Long Term Contract Description: Must have skills SQL and Tableau 4 years of Data Analysis and business knowledge within a healthcare environment and testing arena3-5 years of Microsoft SQL experience specifically using SQL to build queries, reports.--\n",
      "Work Mode: Hybrid\n",
      "Fit Label: No Fit\n",
      "Similarity Score: 0.49\n",
      "--------------------------------------------------\n",
      "\n",
      "5. Role: Top 3 Skills: Data Quality Model, Working with Matrix, Data AnalysisPittsburgh, PA, is the primary location, but we will also consider Lake Mary, FL Data Governance and Data Quality with Testing experience in the field is preferred. Team Size is eight members, and looking for new expansions.Preferred financial services but not looking for a big client. But the small client financial services experience is welcome. Equal Opportunity EmployerVeteransDisabled Benefits include medical, dental, vision, term life insurance, short-term disability insurance, additional voluntary, commuter benefits, and a 401K plan. Our program allows employees to choose the type of coverage that meets their individual needs. Available paid leave may include Paid Sick Leave, where required by law; any other paid leave required by Federal, State, or local law; and Holiday pay upon meeting eligibility criteria. Disclaimer: These benefit offerings do not apply to client-recruited jobs and jobs which are direct hires to a client To read our Candidate Privacy Information Statement, which explains how we will use your information, please visit https:www.modis.comen-uscandidate-privacy The Company will consider qualified applicants with arrest and conviction records.\n",
      "Location: Pittsburgh, PA ( 3 days Week) Client: BNY Mellon Job Id: 70732-1Job Duration: 4-6 Month CTH Enterprises Data Governance Under Data Quality project.Regulatory reporting and KYC Screening is part of the project.Data Quality rules working with Data Lakes.Analytics Data is in the play.Min 8-12 years of working experience in the Data Quality field. Tools: Collibra, Jira, Confluence, and Soft skills in SQL is good.The location is Pittsburgh, PA, for Three days Week in the Hybrid model.One round of Interviews.Data Analyst Business Analyst kind of role:Top 3 Skills: Data Quality Model, Working with Matrix, Data AnalysisPittsburgh, PA, is the primary location, but we will also consider Lake Mary, FL Data Governance and Data Quality with Testing experience in the field is preferred. Team Size is eight members, and looking for new expansions.Preferred financial services but not looking for a big client. But the small client financial services experience is welcome. Equal Opportunity EmployerVeteransDisabled Benefits include medical, dental, vision, term life insurance, short-term disability insurance, additional voluntary, commuter benefits, and a 401K plan. Our program allows employees to choose the type of coverage that meets their individual needs. Available paid leave may include Paid Sick Leave, where required by law; any other paid leave required by Federal, State, or local law; and Holiday pay upon meeting eligibility criteria. Disclaimer: These benefit offerings do not apply to client-recruited jobs and jobs which are direct hires to a client To read our Candidate Privacy Information Statement, which explains how we will use your information, please visit https:www.modis.comen-uscandidate-privacy The Company will consider qualified applicants with arrest and conviction records.\n",
      "Work Mode: Hybrid\n",
      "Fit Label: No Fit\n",
      "Similarity Score: 0.48\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting the system...\")\n",
    "test_query = \"I am skilled in Python and SQL with 5 years of experience. Looking for a job in New York\"\n",
    "results = rag_system.search_jobs(test_query, num_results=5)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "for i, job in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. Role: {job['role']}\")\n",
    "    print(f\"Location: {job['location']}\")\n",
    "    print(f\"Work Mode: {job['work_mode']}\")\n",
    "    print(f\"Fit Label: {job['fit_label']}\")\n",
    "    print(f\"Similarity Score: {job['similarity_score']:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
